# Activity 8
## Due: 9am on February 14, 2025

## Objectives
- Learn about common activation functions and their properties
- Understand why activation functions are crucial for deep learning

## Task
1. Update the Python file to implement ReLU, Sigmoid, and Tanh from scratch.  
3. Run the program to observe the plots of the activation functions and discuss their behavior.

## Common Activation Functions
- **ReLU:** $f(x) = \max(0, x)$
- **Sigmoid:** $f(x) = \frac{1}{1 + e^{-x}}$
- **Tanh:** $f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$

## Run the Program
- Execute `activation_functions.py` to see the plots.  

## Observations
TODO: 
1. Explain how each activation function behaves for positive and negative inputs.
2. Observe which function might be most suitable for deep networks and explain why.
